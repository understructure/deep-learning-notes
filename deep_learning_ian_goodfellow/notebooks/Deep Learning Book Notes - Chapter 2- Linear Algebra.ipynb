{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2- Linear Algebra\n",
    "\n",
    "* linear algebra is a form of continuous rather thandiscrete mathematics\n",
    "* **scalar** - single number\n",
    "\t* matrix with only one entry\n",
    "\t* a scalar is its own transpose\n",
    "* **vector** - array of numbers\n",
    "\t* think of vectors as identifying points in space, with each elementgiving the coordinate along a diﬀerent axis\n",
    "\t* matrix with only one column\n",
    "* **matrix** - a 2-D array of numbers\n",
    "\t* can add matrices of same shape by just adding each element i,j\n",
    "\t* can multiply matrix by scalar\n",
    "\t* can add vector to a matrix, add the vector to each row of the matrix\n",
    "\t* broadcasting - copying a vector to many locations like in adding to a matrix\n",
    "* **tensor** - an array with more than two axes\n",
    "* **transpose** - mirror image of the matrix across a diagonal line, called the main diagonal\n",
    "* $(A^T)_{i,j}$ = $A_{j,i}$\n",
    "* **element-wise (Hadamard) product** - $\\odot$ results in a matrix with the product of the individual elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22, 28],\n",
       "       [49, 64]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# multiplying a 2x3 matrix by a 3x2 matrix produces a 2x2 matrix\n",
    "np.dot([[1,2,3],[4,5,6]], [[1,2],[3,4],[5,6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* matrix multiplication / dot product ($\\cdot$) is:\n",
    "\t* distributive: A(B + C) = AB + AC\n",
    "\t* associative: A(BC) = (AB)C\n",
    "\t* NOT commutative, i.e. AB != BA, \n",
    "    * however dot product b/t two vectors is commutative: $X^TY$ = $Y^TX$\n",
    "* Transpose of a matrix product: $(AB)^T$ = $B^TA^T$\n",
    "* Matrix inverse of $A$ is $A^{-1}$\n",
    "* $A^{-1}A = I$\n",
    "* $A^{-1}$ is primarily a theoretical tool\n",
    "* **column space** of a matrix is number of columns that are linearly independent (e.g., two columns with the same values count as only one column in column space)\n",
    "* Dimensionality of the column space - The number of rows / cases must be greater than or equal to the number of linearly independent columns.\n",
    "* **linear independence** - a set of vectors is linearly independent if no vector in the set is a linear combination of the other vectors\n",
    "* For the column space of the matrix to encompass all of $\\mathbb{R}^m$, the matrix must contain at least one set of $m$ linearly independent columns. This condition is both necessary and suﬃcient for the equation $Ax = b$ to have a solution forevery value of $b$.\n",
    "* **singular matrix** - a square matrix with linearly dependent columns\n",
    "* **scalar matrix** - a square diagonal matrix with all its main diagonal entries equal\n",
    "\n",
    "### 2.5 Norms\n",
    "\n",
    "* Euclidian Norm - $L^2$ norm - $||x||_2$\n",
    "* Used so often in ML sometimes just written $||x||$ without the subscript 2\n",
    "* The norm of a vector $x$ measures the distance from the origin to the point $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7416573867739413"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "p = 2 # L2 norm\n",
    "x = np.array([-1,2,3])\n",
    "# one way to get L2 Norm\n",
    "np.linalg.norm(x, ord=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7416573867739413"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way\n",
    "sum([y ** p for y in x])**(1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7416573867739413"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yet another way is just x transpose * x:\n",
    "\n",
    "sum(x.T * x) ** (1/p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Squared $L^2$ norm is actually easier to work with than the actual $L^2$ norm, because each derivative of thesquared $L^2$ norm with respect to each element of $x$ depends only on the corresponding element of $x$, while all the derivatives of the $L^2$ norm depend on the entire vector\n",
    "    * This isn't desirable sometimes as it increases super slowly around 0, and we're interested in things are are small but nonzero, so in these cases we turn to the $L^1$ norm, which is just the sum of the aboslute values of the elements in the vector:\n",
    "\n",
    "$||x||_1 =\\sum_i |x_i|$\n",
    "\n",
    "* $L^1$ norm is commonly used in machine learning when the diﬀerence between zero and nonzero elements is very important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 1 # L1 norm\n",
    "x = np.array([-1,2,3])\n",
    "# one way to get L2 Norm\n",
    "np.linalg.norm(x, ord=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **max norm** - $L^\\infty$ norm - absolute value of the element with the largest magnitude in the vector\n",
    "* **Frobenius norm** - measures the size of a matrix, analogous to the $L^2$ norm for a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2],[3,4],[5,6]])\n",
    "b = np.array([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.539392014169456"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frobenius norm of a\n",
    "np.linalg.norm(a, 'fro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.539392014169456"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frobenius norm of b is the same because elements of the matrix are the same\n",
    "np.linalg.norm(b, 'fro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product of two vectors can be rewritten in terms of norms. Speciﬁcally,\n",
    "\n",
    "$x^Ty = ||x||_2 ||y||_2 cos\\theta$\n",
    "\n",
    "where $\\theta$ is the angle between the two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Special Kinds of Matrices and Vectors\n",
    "\n",
    "* **diagonal matrix** - 0's everywhere but on the diagonal.  All identity matrices are diagonal matrices\n",
    "* to compute $diag(v)x$, we only need to scale each element $x_i$ by $v_i$. In other words:\n",
    "\n",
    "$diag(v)x=v \\odot x$\n",
    "\n",
    "* inverse of a square diagonal matrix exists if all diagonal elements are non-zero\n",
    "* if this is true, the inverse is another square diagonal matrix, where each value $v_i$ in the diagonal becomes $\\frac{1}{v_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0,  0,  0],\n",
       "       [ 0,  5,  0,  0],\n",
       "       [ 0,  0,  8,  0],\n",
       "       [ 0,  0,  0, 10]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "square_diagonal = np.array([[1,0,0,0], [0,5,0,0], [0,0,8,0], [0,0,0,10]])\n",
    "square_diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.2  , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.125, 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.1  ]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(square_diagonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **symmetric matrix** - any matrix that is equal to its own transpose, e.g., $A = A^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 2, 0, 0],\n",
       "       [0, 0, 3, 0]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rectangular_diagonal1 = np.array([[1,0,0,0], [0,2,0,0], [0,0,3,0]])\n",
    "rectangular_diagonal1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0],\n",
       "       [0, 4, 0, 0],\n",
       "       [0, 0, 6, 0]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rectangular_diagonal1 * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **unit vector** - vector with **unit norm** $||x||_2 = 1$\n",
    "* Two vectors are **orthogonal** to each other if $x^Ty = 0$\n",
    "    * Also means they're at right angles to each other\n",
    "    * If they're orthogonal and of unit length we say they're **orthonormal**\n",
    "* An **orthogonal matrix** is a square matrix whose **rows** are mutually orthonormal **and** whose **columns** are mutually orthonormal:\n",
    "\n",
    "$$A^TA = AA^T = I$$\n",
    "\n",
    "* NOTE: rows are not merely orthogonal but **fully orthonormal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Eigendecomposition\n",
    "\n",
    "* **eigendecomposition** - where we decompose a matrix into a set of eigenvectors and eigenvalues\n",
    "* **eigenvector** of a square matrix is a nonzero vector $v$ such that multiplication by $A$ modifies only the scale of $v$ - $Av = \\lambda V$\n",
    "* $\\lambda$ is a scalar known as the **eigenvalue** corresponding to the eigenvector\n",
    "* usually look only for unit eigenvectors\n",
    "* We can make a matrix with one eigenvector per column: $V$ = $v^{(1)}$ ... $v^{(n)}$\n",
    "* And also a separate vector for all the eigenvectors $\\lambda = \\lambda_1 ... \\lambda_n$\n",
    "* **eigendecomposition** is then given by: $A = V diag(\\lambda)V^{-1}$\n",
    "* Decomposing matrices into their eigenvalues and eigenvectors can help us analyze certain properties of the matrix, much as decomposing an integer into its prime factors can help us understand the behavior of that integer\n",
    "* A matrix is singular ONLY if at least one of the eigenvalues is zero\n",
    "* **positive definite** - all eigenvalues are positive\n",
    "* **positive semidefinite** - all eigenvalues are >= 0\n",
    "* **negative definite** - all eigenvalues are negative\n",
    "* **negative semidefinite**  - all eigenvalues are <= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Singular Value Decomposition\n",
    "\n",
    "* singular value decomposition (SVD) provides another way to factorize a matrix, into **singular vectors** and **singular values**.\n",
    "* every real matrix has a SVD, but not necessarily an eigenvalue decomposition\n",
    "    * e.g., non-square matrices don't have an eigenvalue decomposition, need to do SVD instead\n",
    "* **eigendecomposition**: $A = V diag(\\lambda)V^{-1}$\n",
    "* **SVD**:  $A = UDV^T$ where\n",
    "    * $A$ is an m x n matrix\n",
    "    * $U$ is an m x m matrix and is orthogonal and its columns are **left-singular vectors**\n",
    "    * $D$ is an m x n matrix and is diagonal (but not necessarily square) where the elements on the diagonal are the **singular values** of $A$\n",
    "    * $V$ is an n x n matrix and is orthogonal and its columns are **right-singular vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 The Moore-Penrose Pseudoinverse\n",
    "\n",
    "* When looking for an inverse of a nonsquare matrix:\n",
    "    * If a matrix is taller than it is wide, it's possible there's no solution\n",
    "    * If a matrix is wider than it is tall, it's possible that there are multiple solutions\n",
    "* Reread this, not sticking at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 The Trace Operator\n",
    "\n",
    "* Gives the sum of all diagonal entries of a matrix\n",
    "* Even if two matrices are m x n and n x m: $Tr(AB) = Tr(BA)$ even though the shape of the result of the dot product of $AB$ would be different than the shape of $BA$ (provided m != n)\n",
    "* A scalar is its own trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 The Determinant\n",
    "\n",
    "* **the determinant** is equal to the product of all the eigenvalues in a matrix\n",
    "* Think of it \"as a measure of how much multiplication by the matrix expands or contracts space\"\n",
    "    * If $det(A)$ is 0, space is completely contracted along at least one dimension\n",
    "    * If $det(A)$ is 1, transformation preserves the volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 Example: Principal Components Analysis\n",
    "\n",
    "* PCA can be derived with only linear algebra\n",
    "* Uses the $L^2$ norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
